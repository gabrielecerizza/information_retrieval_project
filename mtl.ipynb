{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\r\n",
                "import spacy\r\n",
                "import json\r\n",
                "import numpy as np\r\n",
                "from tqdm.notebook import tqdm\r\n",
                "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
                "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\r\n",
                "\r\n",
                "from irproject.historical_events import (\r\n",
                "    tag_paragraph, preprocess_data, load_data\r\n",
                ")\r\n",
                "from transformers import (\r\n",
                "    BertTokenizerFast, BertForTokenClassification, BertForPreTraining, BertForMaskedLM,\r\n",
                "    BertModel, BertForSequenceClassification, BertTokenizer, AdamW\r\n",
                ")\r\n",
                "\r\n",
                "spacy.prefer_gpu()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "False"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 1
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "nlp = spacy.load(\"en_core_web_sm\")\r\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "with open(\r\n",
                "    f\"datasets/historical_events/wiki_dataset.json\",\r\n",
                "    encoding=\"utf-8\"\r\n",
                ") as f_in:\r\n",
                "    dataset = json.load(f_in)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "texts, tags, labels = load_data(dataset, nlp)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "train_texts, test_texts, train_tags, test_tags, train_labels, test_labels = train_test_split(\r\n",
                "    texts, tags, labels, test_size=0.2, random_state=42, stratify=labels\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "train_texts, valid_texts, train_tags, valid_tags, train_labels, valid_labels = train_test_split(\r\n",
                "    train_texts, train_tags, train_labels, test_size=0.2, random_state=42, stratify=train_labels\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "encodings, tokens_labels, labels, tag2idx, idx2tag = preprocess_data(\r\n",
                "    train_texts.tolist(), train_tags.tolist(), \r\n",
                "    train_labels.tolist(), tokenizer, padding=\"max_length\"\r\n",
                ")"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(HTML(value='Adjusting tags to encodings'), FloatProgress(value=1.0, bar_style='info', layout=La…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "06b7813d471b426696f974a121426f37"
                        }
                    },
                    "metadata": {}
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "torch.save(encodings.input_ids, \"datasets/historical_events/train/input_ids.pkl\")\r\n",
                "torch.save(encodings.attention_mask, \"datasets/historical_events/train/attention_mask.pkl\")\r\n",
                "torch.save(tokens_labels, \"datasets/historical_events/train/tokens_labels.pkl\")\r\n",
                "torch.save(labels, \"datasets/historical_events/train/labels.pkl\")\r\n",
                "torch.save(tag2idx, \"datasets/historical_events/train/tag2idx.pkl\")\r\n",
                "torch.save(idx2tag, \"datasets/historical_events/train/idx2tag.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "input_ids = torch.load(\"datasets/historical_events/train/input_ids.pkl\")\r\n",
                "tokens_labels = torch.load(\"datasets/historical_events/train/tokens_labels.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "train_data = TensorDataset(\r\n",
                "    torch.tensor(encodings.input_ids, device=device), \r\n",
                "    torch.tensor(encodings.attention_mask, device=device), \r\n",
                "    torch.tensor(tokens_labels, device=device),\r\n",
                "    torch.tensor(labels, device=device, dtype=torch.float32)\r\n",
                ")\r\n",
                "train_loader = DataLoader(\r\n",
                "    train_data, shuffle=True, batch_size=1\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "source": [
                "train_loader = DataLoader(\r\n",
                "    train_data, shuffle=True, batch_size=128\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "source": [
                "torch.save(train_loader, \"datasets/historical_events/train.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "train_loader = torch.load(\"datasets/historical_events/train.pkl\")\r\n",
                "valid_loader = torch.load(\"datasets/historical_events/valid.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "encodings, tokens_labels, labels, tag2idx, idx2tag = preprocess_data(\r\n",
                "    valid_texts.tolist(), valid_tags.tolist(), \r\n",
                "    valid_labels.tolist(), tokenizer, padding=\"max_length\"\r\n",
                ")"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(HTML(value='Adjusting tags to encodings'), FloatProgress(value=1.0, bar_style='info', layout=La…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "f7b6ccc45e4249e48815303475c87a6a"
                        }
                    },
                    "metadata": {}
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "torch.save(encodings.input_ids, \"datasets/historical_events/valid/input_ids.pkl\")\r\n",
                "torch.save(encodings.attention_mask, \"datasets/historical_events/valid/attention_mask.pkl\")\r\n",
                "torch.save(tokens_labels, \"datasets/historical_events/valid/tokens_labels.pkl\")\r\n",
                "torch.save(labels, \"datasets/historical_events/valid/labels.pkl\")\r\n",
                "torch.save(tag2idx, \"datasets/historical_events/valid/tag2idx.pkl\")\r\n",
                "torch.save(idx2tag, \"datasets/historical_events/valid/idx2tag.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "valid_data = TensorDataset(\r\n",
                "    torch.tensor(encodings.input_ids, device=device), \r\n",
                "    torch.tensor(encodings.attention_mask, device=device), \r\n",
                "    torch.tensor(tokens_labels, device=device),\r\n",
                "    torch.tensor(labels, device=device, dtype=torch.float32)\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "source": [
                "valid_loader = DataLoader(\r\n",
                "    valid_data, shuffle=True, batch_size=128\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "source": [
                "torch.save(valid_loader, \"datasets/historical_events/valid.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "encodings, tokens_labels, labels, tag2idx, idx2tag = preprocess_data(\r\n",
                "    test_texts.tolist(), test_tags.tolist(), \r\n",
                "    test_labels.tolist(), tokenizer, padding=\"max_length\"\r\n",
                ")\r\n"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(HTML(value='Adjusting tags to encodings'), FloatProgress(value=1.0, bar_style='info', layout=La…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "9a6e3bca18ec49e5807b109850d6dbe5"
                        }
                    },
                    "metadata": {}
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "torch.save(encodings.input_ids, \"datasets/historical_events/test/input_ids.pkl\")\r\n",
                "torch.save(encodings.attention_mask, \"datasets/historical_events/test/attention_mask.pkl\")\r\n",
                "torch.save(tokens_labels, \"datasets/historical_events/test/tokens_labels.pkl\")\r\n",
                "torch.save(labels, \"datasets/historical_events/test/labels.pkl\")\r\n",
                "torch.save(tag2idx, \"datasets/historical_events/test/tag2idx.pkl\")\r\n",
                "torch.save(idx2tag, \"datasets/historical_events/test/idx2tag.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "test_data = TensorDataset(\r\n",
                "    torch.tensor(encodings.input_ids, device=device), \r\n",
                "    torch.tensor(encodings.attention_mask, device=device), \r\n",
                "    torch.tensor(tokens_labels, device=device),\r\n",
                "    torch.tensor(labels, device=device, dtype=torch.float32)\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "source": [
                "test_loader = DataLoader(\r\n",
                "    test_data, shuffle=True, batch_size=128\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "source": [
                "torch.save(test_loader, \"datasets/historical_events/test.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "bert = BertModel.from_pretrained(\"bert-base-cased\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
                        "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "import torch.nn.functional as F\r\n",
                "from torch import nn\r\n",
                "\r\n",
                "class MultiTaskLearningModel(nn.Module):\r\n",
                "\r\n",
                "    def __init__(self, base_model, dropout_rates, hidden_size, num_labels):\r\n",
                "        super(MultiTaskLearningModel, self).__init__()\r\n",
                "        self.base_model = base_model\r\n",
                "\r\n",
                "        # We could avoid sigmoid here and use the\r\n",
                "        # BCEWithLogitsLoss, which computes both the sigmoid\r\n",
                "        # and the BCE with a trick for numerical stability.\r\n",
                "        self.seq_clf = nn.Sequential(\r\n",
                "            nn.Dropout(p=dropout_rates[0]),\r\n",
                "            nn.Linear(in_features=768, out_features=hidden_size),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Dropout(p=dropout_rates[1]),\r\n",
                "            nn.Linear(in_features=hidden_size, out_features=1),\r\n",
                "            nn.Sigmoid()\r\n",
                "        )\r\n",
                "\r\n",
                "        self.tokens_clf = nn.Sequential(\r\n",
                "            nn.Dropout(p=dropout_rates[2]),\r\n",
                "            nn.Linear(hidden_size, num_labels)\r\n",
                "            # nn.LogSoftmax(dim=1)\r\n",
                "        )\r\n",
                "\r\n",
                "        for p in self.seq_clf.parameters():\r\n",
                "            if p.dim() > 1:\r\n",
                "                nn.init.xavier_uniform_(p)\r\n",
                "\r\n",
                "        for p in self.tokens_clf.parameters():\r\n",
                "            if p.dim() > 1:\r\n",
                "                nn.init.xavier_uniform_(p)\r\n",
                "        \r\n",
                "    def forward(self, input_ids, attention_mask):\r\n",
                "        output = self.base_model(\r\n",
                "            input_ids=input_ids,\r\n",
                "            attention_mask=attention_mask\r\n",
                "        )\r\n",
                "\r\n",
                "        last_hidden_state = output.last_hidden_state\r\n",
                "\r\n",
                "        # TODO Since we are using mean, maybe it's better not to pad to max?\r\n",
                "        seq_clf_out = self.seq_clf(torch.mean(last_hidden_state, dim=1))\r\n",
                "        tokens_clf_out = self.tokens_clf(last_hidden_state)\r\n",
                "\r\n",
                "        return seq_clf_out, tokens_clf_out\r\n",
                "\r\n",
                "    def freeze_base(self):\r\n",
                "        for param in self.base_model.named_parameters():\r\n",
                "            param[1].requires_grad=False\r\n",
                "\r\n",
                "    def unfreeze_base(self):\r\n",
                "        for param in self.base_model.named_parameters():\r\n",
                "            param[1].requires_grad=True\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "source": [
                "from torch.nn import BCELoss, CrossEntropyLoss\r\n",
                "from torch import nn\r\n",
                "\r\n",
                "class MultiTaskLoss(nn.Module):\r\n",
                "    \"\"\"Uncertainty weighted loss for multi-task learning\r\n",
                "    proposed in Kendall et al., Multi-Task Learning Using \r\n",
                "    Uncertainty to Weigh Losses for Scene Geometry and \r\n",
                "    Semantics, arXiv:1705.07115v3.\r\n",
                "\r\n",
                "    Possible alternative implementations are available\r\n",
                "    at:\r\n",
                "        - https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example-pytorch.ipynb\r\n",
                "        - https://github.com/lorenmt/mtan/blob/master/im2im_pred/utils.py\r\n",
                "    \"\"\"\r\n",
                "\r\n",
                "    def __init__(self, losses_num: int = 2, num_tokens_labels: int = 5):\r\n",
                "        super(MultiTaskLoss, self).__init__()\r\n",
                "        self.losses_num = losses_num\r\n",
                "        self.num_tokens_labels = num_tokens_labels\r\n",
                "        self.log_vars = nn.Parameter(torch.zeros((losses_num)))\r\n",
                "\r\n",
                "    def forward(self, seq_clf_out, tokens_clf_out, labels, tokens_labels, attention_mask):\r\n",
                "\r\n",
                "        loss_ce = CrossEntropyLoss()\r\n",
                "        loss_bce = BCELoss()\r\n",
                "\r\n",
                "        if attention_mask is not None:\r\n",
                "            active_loss = attention_mask.view(-1) == 1\r\n",
                "            active_logits = tokens_clf_out.view(-1, self.num_tokens_labels)\r\n",
                "            active_labels = torch.where(\r\n",
                "                active_loss, \r\n",
                "                tokens_labels.view(-1), \r\n",
                "                torch.tensor(loss_ce.ignore_index).type_as(tokens_labels)\r\n",
                "            )\r\n",
                "            loss0 = loss_ce(active_logits, active_labels)\r\n",
                "        else:\r\n",
                "            loss0 = loss_ce(\r\n",
                "                tokens_clf_out.view(-1, self.num_tokens_labels), \r\n",
                "                tokens_labels.view(-1)\r\n",
                "            )\r\n",
                "\r\n",
                "        loss1 = loss_bce(seq_clf_out.view(-1), labels.view(-1))\r\n",
                "\r\n",
                "        losses = [loss0, loss1]\r\n",
                "\r\n",
                "        loss = sum(\r\n",
                "            torch.exp(-self.log_vars[i]) * losses[i] + (self.log_vars[i] / 2)\r\n",
                "            for i in range(self.losses_num)\r\n",
                "        )\r\n",
                "        \r\n",
                "        return loss"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# WE NEED TO SET -100 on all \"O\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "num_epochs = 3\r\n",
                "max_grad_norm = 1.0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "\r\n",
                "\r\n",
                "def tensor_accuracy(y_true, y_preds):\r\n",
                "\r\n",
                "    # The model outputs probabilities for each token in the\r\n",
                "    # sentence. So the output for the tokens will be of size\r\n",
                "    # (#batches, #tokens, #classes). Method torch.max returns\r\n",
                "    # the max value and the index of the max value. So we can get\r\n",
                "    # the class predicted for each token in the batch.\r\n",
                "\r\n",
                "    # _, y_preds = torch.max(y_preds_probas, -1)\r\n",
                "    y_correct = (y_preds == y_true).sum().detach()\r\n",
                "    acc = y_correct / y_true.size(0)\r\n",
                "\r\n",
                "    return acc\r\n",
                "    \r\n",
                "\r\n",
                "def tensor_binary_accuracy(y_true, y_preds_probas):\r\n",
                "    y_preds = (y_preds_probas > 0.5).view(1, -1)\r\n",
                "    y_correct = (y_preds == y_true).sum().detach()\r\n",
                "    acc = y_correct / y_true.size(0)\r\n",
                "\r\n",
                "    return acc\r\n",
                "\r\n",
                "    preds_batch_np = np.round(probas.cpu().detach().numpy())\r\n",
                "    y_batch_np = y_batch.cpu().detach().numpy()\r\n",
                "    acc = accuracy_score(y_true=y_batch_np, y_pred=preds_batch_np)\r\n",
                "    f1 = f1_score(y_true=y_batch_np, y_pred=preds_batch_np, average='weighted')\r\n",
                "    return acc, f1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "from torch.optim import lr_scheduler\r\n",
                "from sklearn.metrics import accuracy_score, f1_score\r\n",
                "\r\n",
                "model = MultiTaskLearningModel(bert, [0.1, 0.3, 0.1], 768, 6)\r\n",
                "model.to(device)\r\n",
                "model.train()\r\n",
                "\r\n",
                "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\r\n",
                "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=2)\r\n",
                "criterion = MultiTaskLoss(2, 6).to(device)\r\n",
                "\r\n",
                "train_loss_values, validation_loss_values = [], []\r\n",
                "freeze_epochs = [0, 1, 2]\r\n",
                "unfreeze_epochs = [3, 4]\r\n",
                "\r\n",
                "for epoch in tqdm(\r\n",
                "    range(num_epochs),\r\n",
                "    desc=\"Epoch\",\r\n",
                "    leave=False\r\n",
                "):\r\n",
                "    print(\"=== Epoch: \", epoch, \" / \", num_epochs, \" ===\")\r\n",
                "    print(\"LR:\", optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\r\n",
                "    if epoch in freeze_epochs:\r\n",
                "        print(\"Freeze base model\")\r\n",
                "        model.freeze_base()\r\n",
                "            \r\n",
                "    if epoch in unfreeze_epochs:\r\n",
                "        print(\"Unfreeze base model\")\r\n",
                "        model.unfreeze_base()\r\n",
                "\r\n",
                "    train_loss = 0\r\n",
                "    valid_loss = 0\r\n",
                "    best_f1 = 0\r\n",
                "    history = []\r\n",
                "\r\n",
                "    # Seqc = classification for the whole sequence.\r\n",
                "    # Tokc = token classification (NER).\r\n",
                "    train_seqc_labels, train_seqc_preds = [], []\r\n",
                "    train_tokc_labels, train_tokc_preds = [], []\r\n",
                "\r\n",
                "    valid_seqc_labels, valid_seqc_preds = [], []\r\n",
                "    valid_tokc_labels, valid_tokc_preds = [], []\r\n",
                "\r\n",
                "    # Training.\r\n",
                "    model.train()\r\n",
                "    for step, batch in tqdm(\r\n",
                "        enumerate(train_loader),\r\n",
                "        desc=\"Training step\",\r\n",
                "        leave=False\r\n",
                "    ):\r\n",
                "        if step == 3:\r\n",
                "            break\r\n",
                "\r\n",
                "        model.zero_grad()\r\n",
                "\r\n",
                "        input_ids, attention_mask, tokens_labels, labels = batch\r\n",
                "\r\n",
                "        seqc_out, tokc_out = model(input_ids, attention_mask)\r\n",
                "        loss = criterion(seqc_out, tokc_out, labels, tokens_labels, attention_mask)\r\n",
                "        loss.backward()\r\n",
                "        train_loss += loss.detach()\r\n",
                "\r\n",
                "        torch.nn.utils.clip_grad_norm_(\r\n",
                "            parameters=model.parameters(),\r\n",
                "            max_norm=max_grad_norm\r\n",
                "        )\r\n",
                "\r\n",
                "        optimizer.step()\r\n",
                "\r\n",
                "        train_seqc_labels.extend(labels)\r\n",
                "        train_tokc_labels.extend(tokens_labels)\r\n",
                "        train_seqc_preds.extend(seqc_out)\r\n",
                "        train_tokc_preds.extend(torch.argmax(tokc_out, dim=-1).view(-1))\r\n",
                "\r\n",
                "    train_seqc_labels = torch.tensor(train_seqc_labels, device=device)\r\n",
                "    train_tokc_labels = torch.cat(train_tokc_labels)\r\n",
                "    train_seqc_preds = (torch.tensor(train_seqc_preds, device=device) > 0.5)\r\n",
                "    train_tokc_preds = torch.tensor(train_tokc_preds, device=device)\r\n",
                "\r\n",
                "    # Validation.\r\n",
                "    model.eval()\r\n",
                "    for step, batch in tqdm(\r\n",
                "        enumerate(valid_loader),\r\n",
                "        desc=\"Validation step\",\r\n",
                "        leave=False\r\n",
                "    ):\r\n",
                "        if step == 3:\r\n",
                "            break\r\n",
                "\r\n",
                "        input_ids, attention_mask, tokens_labels, labels = batch\r\n",
                "\r\n",
                "        with torch.no_grad():\r\n",
                "            seqc_out, tokc_out = model(input_ids, attention_mask)\r\n",
                "            loss = criterion(seqc_out, tokc_out, labels, tokens_labels, attention_mask)\r\n",
                "            valid_loss += loss.detach()\r\n",
                "\r\n",
                "            valid_seqc_labels.extend(labels)\r\n",
                "            valid_tokc_labels.extend(tokens_labels)\r\n",
                "            valid_seqc_preds.extend(seqc_out)\r\n",
                "            valid_tokc_preds.extend(torch.argmax(tokc_out, dim=-1).view(-1))\r\n",
                "\r\n",
                "    valid_seqc_labels = torch.tensor(valid_seqc_labels, device=device)\r\n",
                "    valid_tokc_labels = torch.cat(valid_tokc_labels)\r\n",
                "    valid_seqc_preds = (torch.tensor(valid_seqc_preds, device=device) > 0.5)\r\n",
                "    valid_tokc_preds = torch.tensor(valid_tokc_preds, device=device)\r\n",
                "\r\n",
                "    train_avg_loss = train_loss / len(train_loader)\r\n",
                "    valid_avg_loss = valid_loss / len(valid_loader)\r\n",
                "    train_seqc_acc = tensor_binary_accuracy(\r\n",
                "        train_seqc_labels, train_seqc_preds\r\n",
                "    )\r\n",
                "    train_tokc_acc = tensor_accuracy(train_tokc_labels, train_tokc_preds)\r\n",
                "    train_seqc_f1 = f1_score(train_seqc_labels.cpu(), train_seqc_preds.cpu())\r\n",
                "    train_tokc_f1 = f1_score(train_tokc_labels.cpu(), train_tokc_preds.cpu(), average=\"weighted\")\r\n",
                "\r\n",
                "    valid_seqc_acc = tensor_binary_accuracy(\r\n",
                "        valid_seqc_labels, valid_seqc_preds\r\n",
                "    )\r\n",
                "    valid_tokc_acc = tensor_accuracy(valid_tokc_labels, valid_tokc_preds)\r\n",
                "    valid_seqc_f1 = f1_score(valid_seqc_labels.cpu(), valid_seqc_preds.cpu())\r\n",
                "    valid_tokc_f1 = f1_score(valid_tokc_labels.cpu(), valid_tokc_preds.cpu(), average=\"weighted\")\r\n",
                "\r\n",
                "    print(\"Train avg loss\", train_avg_loss)\r\n",
                "    print(\"Valid avg loss\", valid_avg_loss)\r\n",
                "    print(\"Train seqc accuracy:\", train_seqc_acc)\r\n",
                "    print(\"Train tokc accuracy:\", train_tokc_acc)\r\n",
                "    print(\"Train seqc f1-score:\", train_seqc_f1)\r\n",
                "    print(\"Train tokc f1-score:\", train_tokc_f1)\r\n",
                "    print(\"Valid seqc accuracy:\", valid_seqc_acc)\r\n",
                "    print(\"Valid tokc accuracy:\", valid_tokc_acc)\r\n",
                "    print(\"Valid seqc f1-score:\", valid_seqc_f1)\r\n",
                "    print(\"Valid tokc f1-score:\", valid_tokc_f1)\r\n",
                "    if valid_seqc_f1 > best_f1:\r\n",
                "        best_f1 = valid_seqc_f1\r\n",
                "        torch.save(model, \"best_mtl_model.pt\")\r\n",
                "\r\n",
                "    history.append(\r\n",
                "        {\r\n",
                "            \"train_seqc_acc\": train_seqc_acc, \r\n",
                "            \"train_tokc_acc\": train_tokc_acc,\r\n",
                "            \"train_seqc_f1\": train_seqc_f1, \r\n",
                "            \"train_tokc_f1\": train_tokc_f1, \r\n",
                "            \"valid_seqc_acc\": valid_seqc_acc, \r\n",
                "            \"valid_tokc_acc\": valid_tokc_acc,\r\n",
                "            \"valid_seqc_f1\": valid_seqc_f1, \r\n",
                "            \"valid_tokc_f1\": valid_tokc_f1, \r\n",
                "        }\r\n",
                "    )\r\n",
                "\r\n",
                "    scheduler.step(valid_avg_loss)\r\n",
                "    break\r\n"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "775ecf4042624db9897995e53adb39a8"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "=== Epoch:  0  /  3  ===\n",
                        "LR: 2e-05\n",
                        "Freeze base model\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(HTML(value='Training step'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "8cb1d4c9bc8e4e3a9e299d69310f198c"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(HTML(value='Validation step'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "7edf709573cd42bd98db2dd975cf397c"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Train avg loss tensor(0.0380, device='cuda:0')\n",
                        "Valid avg loss tensor(0.1503, device='cuda:0')\n",
                        "Train seqc accuracy: tensor(0.6120, device='cuda:0')\n",
                        "Train tokc accuracy: tensor(0.0056, device='cuda:0')\n",
                        "Train seqc f1-score: 0.2512562814070351\n",
                        "Train tokc f1-score: 0.0005162976379357603\n",
                        "Valid seqc accuracy: tensor(0.5729, device='cuda:0')\n",
                        "Valid tokc accuracy: tensor(0.0064, device='cuda:0')\n",
                        "Valid seqc f1-score: 0.0\n",
                        "Valid tokc f1-score: 0.0006589554572518321\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "source": [
                "train_seqc_labels"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
                            "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
                            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
                            "        1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 159
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 160,
            "source": [
                "train_seqc_preds"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([0.3689, 0.1840, 0.5953, 0.5562, 0.3302, 0.3291, 0.3773, 0.2022, 0.4021,\n",
                            "        0.6044, 0.7476, 0.4082, 0.5453, 0.4340, 0.7084, 0.6595, 0.6811, 0.7641,\n",
                            "        0.7952, 0.7075, 0.8425, 0.5931, 0.8157, 0.7328, 0.6031, 0.8657, 0.7726,\n",
                            "        0.8644, 0.8393, 0.7330, 0.7311, 0.8303, 0.7542, 0.7391, 0.8119, 0.8245,\n",
                            "        0.8378, 0.8597, 0.7686, 0.8109, 0.8461, 0.9022, 0.9361, 0.8982, 0.7767,\n",
                            "        0.9475, 0.8419, 0.8587, 0.8516, 0.8036, 0.9068, 0.7920, 0.8010, 0.7871,\n",
                            "        0.9416, 0.8206, 0.7776, 0.8981, 0.8875, 0.8977, 0.8617, 0.9382, 0.9478,\n",
                            "        0.9169, 0.8798, 0.9268, 0.9399, 0.8918, 0.9196, 0.9167, 0.9597, 0.9372,\n",
                            "        0.8534, 0.8761, 0.9129, 0.8624, 0.9420, 0.8994, 0.8853, 0.9656, 0.9527,\n",
                            "        0.9619, 0.9463, 0.9455, 0.9462, 0.8733, 0.8719, 0.8756, 0.8060, 0.9374,\n",
                            "        0.9641, 0.9117, 0.9266, 0.8690, 0.8798, 0.8781, 0.9080, 0.9262, 0.9679,\n",
                            "        0.9401, 0.9263, 0.9388, 0.9554, 0.9245, 0.8737, 0.9601, 0.9595, 0.9547,\n",
                            "        0.9087, 0.9168, 0.9068, 0.9027, 0.8737, 0.9547, 0.9590, 0.9460, 0.9602,\n",
                            "        0.9547, 0.9277, 0.9836, 0.9298, 0.9638, 0.9580, 0.9597, 0.9664, 0.9786,\n",
                            "        0.9540, 0.9223, 0.9724, 0.9600, 0.9098, 0.9250, 0.9715, 0.9476, 0.9561,\n",
                            "        0.9698, 0.9791, 0.9541, 0.9688, 0.9766, 0.9453, 0.9743, 0.9796, 0.9903,\n",
                            "        0.9786, 0.9829, 0.9706, 0.9377, 0.9713, 0.9860, 0.9811, 0.9890, 0.9741,\n",
                            "        0.9621, 0.9514, 0.9774, 0.9913, 0.9627, 0.9746, 0.9760, 0.9598, 0.9891,\n",
                            "        0.9722, 0.9751, 0.9772, 0.9696, 0.9864, 0.9683, 0.9820, 0.9891, 0.9584,\n",
                            "        0.9706, 0.9670, 0.9631, 0.9852, 0.9830, 0.9876, 0.9711, 0.9448, 0.9821,\n",
                            "        0.9869, 0.9881, 0.9837, 0.9921, 0.9430, 0.9889, 0.9651, 0.9762, 0.9764,\n",
                            "        0.9684, 0.9748, 0.9842, 0.9924, 0.9702, 0.9831, 0.9927, 0.9937, 0.9873,\n",
                            "        0.9878, 0.9752, 0.9882, 0.9644, 0.9891, 0.9832, 0.9786, 0.9852, 0.9805,\n",
                            "        0.9870, 0.9680, 0.9839, 0.9918, 0.9828, 0.9690, 0.9925, 0.9870, 0.9790,\n",
                            "        0.9862, 0.9776, 0.9813, 0.9780, 0.9707, 0.9669, 0.9762, 0.9895, 0.9877,\n",
                            "        0.9838, 0.9611, 0.9883, 0.9646, 0.9750, 0.9727, 0.9745, 0.9905, 0.9878,\n",
                            "        0.9746, 0.9648, 0.9815, 0.9819, 0.9777, 0.9784, 0.9636, 0.9879, 0.9824,\n",
                            "        0.9524, 0.9875, 0.9874, 0.9638, 0.9841, 0.9801, 0.9775, 0.9323, 0.9809,\n",
                            "        0.9765, 0.9554, 0.9588, 0.9930, 0.9911, 0.9810, 0.9751, 0.9824, 0.9763,\n",
                            "        0.9907, 0.9816, 0.9920, 0.9870, 0.9959, 0.9887, 0.9497, 0.9919, 0.9743,\n",
                            "        0.9722, 0.9760, 0.9826, 0.9794, 0.9868, 0.9665, 0.9896, 0.9902, 0.9666,\n",
                            "        0.9771, 0.9841, 0.9754, 0.9847, 0.9872, 0.9729, 0.9752, 0.9888, 0.9856,\n",
                            "        0.9644, 0.9947, 0.9915, 0.9952, 0.9844, 0.9930, 0.9872, 0.9775, 0.9960,\n",
                            "        0.9950, 0.9863, 0.9761, 0.9881, 0.9847, 0.9850, 0.9848, 0.9902, 0.9933,\n",
                            "        0.9810, 0.9882, 0.9925, 0.9735, 0.9750, 0.9903, 0.9880, 0.9803, 0.9805,\n",
                            "        0.9838, 0.9711, 0.9955, 0.9888, 0.9743, 0.9837, 0.9859, 0.9656, 0.9940,\n",
                            "        0.9783, 0.9764, 0.9812, 0.9890, 0.9721, 0.9885, 0.9881, 0.9892, 0.9776,\n",
                            "        0.9931, 0.9833, 0.9732, 0.9895, 0.9852, 0.9677, 0.9680, 0.9803, 0.9855,\n",
                            "        0.9614, 0.9875, 0.9825, 0.9918, 0.9928, 0.9908, 0.9927, 0.9957, 0.9950,\n",
                            "        0.9810, 0.9942, 0.9902, 0.9908, 0.9876, 0.9845, 0.7408, 0.9956, 0.9781,\n",
                            "        0.9909, 0.9861, 0.9774, 0.9874, 0.9838, 0.9755, 0.9840, 0.9619, 0.9689,\n",
                            "        0.9529, 0.9862, 0.9810, 0.9885, 0.9778, 0.9808, 0.9856, 0.9642, 0.9975,\n",
                            "        0.9862, 0.9897, 0.9918, 0.9893, 0.9926, 0.9927, 0.9918, 0.9884, 0.9801,\n",
                            "        0.9985, 0.9921, 0.9933, 0.9900, 0.9941, 0.9974, 0.9965, 0.9952, 0.9903,\n",
                            "        0.9944, 0.9902, 0.9827, 0.9926, 0.9966, 0.9936, 0.9959, 0.9960, 0.9939,\n",
                            "        0.9929, 0.9912, 0.9858, 0.9956, 0.9841, 0.9911, 0.9885, 0.9939, 0.9938,\n",
                            "        0.9961, 0.9933, 0.9952, 0.9908, 0.9876, 0.9918, 0.9950, 0.9943, 0.9896,\n",
                            "        0.9979, 0.9843, 0.9875, 0.9930, 0.9890, 0.9857, 0.9863, 0.9924, 0.9966,\n",
                            "        0.9889, 0.9879, 0.9936, 0.9968, 0.9922, 0.9933, 0.9906, 0.9945, 0.9925,\n",
                            "        0.9856, 0.9805, 0.9961, 0.9963, 0.9963, 0.9956, 0.9972, 0.9951, 0.9955,\n",
                            "        0.9951, 0.9985, 0.9943, 0.9939, 0.9954, 0.9984, 0.9957, 0.9924, 0.9964,\n",
                            "        0.9901, 0.9956, 0.9936, 0.9938, 0.9908, 0.9901, 0.9944, 0.9950, 0.9948,\n",
                            "        0.9926, 0.9898, 0.9957, 0.9965, 0.9929, 0.9979, 0.9853, 0.9969, 0.9978,\n",
                            "        0.9901, 0.9933, 0.9971, 0.9855, 0.9906, 0.9959, 0.9884, 0.9956, 0.9966,\n",
                            "        0.9922, 0.9951, 0.9910, 0.9948, 0.9914, 0.9952, 0.9956, 0.9993, 0.9955,\n",
                            "        0.9972, 0.9872, 0.9903, 0.9947, 0.9935, 0.9954, 0.9974, 0.9969, 0.9955,\n",
                            "        0.9979, 0.9926, 0.9943, 0.9977, 0.9978, 0.9980, 0.9958, 0.9982, 0.9986,\n",
                            "        0.9972, 0.9979, 0.9961, 0.9964, 0.9968, 0.9984, 0.9972, 0.9986, 0.9943,\n",
                            "        0.9862, 0.9961, 0.9981, 0.9958, 0.9965, 0.9901, 0.9816, 0.9946, 0.9974,\n",
                            "        0.9977, 0.9964, 0.9972, 0.9992, 0.9984, 0.9984, 0.9914, 0.9987, 0.9970,\n",
                            "        0.9955, 0.9976, 0.9984, 0.9932, 0.9949, 0.9975, 0.9978, 0.9958, 0.9963,\n",
                            "        0.9972, 0.9950, 0.9841, 0.9971, 0.9957, 0.9980, 0.9937, 0.9963, 0.9978,\n",
                            "        0.9944, 0.9977, 0.9945, 0.9932, 0.9950, 0.9918, 0.9780, 0.9952, 0.9953,\n",
                            "        0.9904, 0.9881, 0.9821, 0.9959, 0.9895, 0.9955, 0.9927, 0.9960, 0.9971,\n",
                            "        0.9981, 0.9933, 0.9899, 0.9967, 0.9965, 0.9974, 0.9967, 0.9961, 0.9927,\n",
                            "        0.9940, 0.9916, 0.9932, 0.9934, 0.9938, 0.9970, 0.9975, 0.9948, 0.9979,\n",
                            "        0.9955, 0.9926, 0.9921, 0.9979, 0.9969, 0.9920, 0.9882, 0.9905, 0.9945,\n",
                            "        0.9958, 0.9974, 0.9975, 0.9968, 0.9955, 0.9956, 0.9980, 0.9941, 0.9973,\n",
                            "        0.9978, 0.9956, 0.9988, 0.9961, 0.9964, 0.9992, 0.9991, 0.9964, 0.9985,\n",
                            "        0.9945, 0.9979, 0.9940, 0.9978, 0.9919, 0.9961, 0.9977, 0.9968, 0.9972,\n",
                            "        0.9980, 0.9966, 0.9946, 0.9950, 0.9988, 0.9918, 0.9956, 0.9984, 0.9989,\n",
                            "        0.9982, 0.9969, 0.9983, 0.9986, 0.9969, 0.9957, 0.9977, 0.9983, 0.9978,\n",
                            "        0.9953, 0.9985, 0.9957, 0.9963, 0.9978, 0.9976, 0.9992, 0.9977, 0.9979,\n",
                            "        0.9993, 0.9943, 0.9992, 0.9985, 0.9987, 0.9994, 0.9984, 0.9989, 0.9985,\n",
                            "        0.9993, 0.9994, 0.9992, 0.9991, 0.9985, 0.9947, 0.9976, 0.9990, 0.9990,\n",
                            "        0.9987, 0.9988, 0.9969, 0.9995, 0.9945, 0.9987, 0.9992, 0.9974, 0.9996,\n",
                            "        0.9994, 0.9994, 0.9992, 0.9984, 0.9986, 0.9992, 0.9992, 0.9993, 0.9955,\n",
                            "        0.9989, 0.9994, 0.9982, 0.9985, 0.9983, 0.9992, 0.9978, 0.9976, 0.9937,\n",
                            "        0.9995, 0.9994, 0.9992, 0.9985, 0.9980, 0.9975, 0.9992, 0.9981, 0.9990,\n",
                            "        0.9977, 0.9984, 0.9964, 0.9938, 0.9980, 0.9952, 0.9981, 0.9979, 0.9988,\n",
                            "        0.9985, 0.9974, 0.9993, 0.9987, 0.9983, 0.9987, 0.9970, 0.9985, 0.9983,\n",
                            "        0.9989, 0.9989, 0.9986, 0.9990, 0.9970, 0.9990, 0.9988, 0.9974, 0.9984,\n",
                            "        0.9991, 0.9988, 0.9949, 0.9945, 0.9950, 0.9984, 0.9981, 0.9984, 0.9920,\n",
                            "        0.9954, 0.9986, 0.9949, 0.9965, 0.9979, 0.9964, 0.9979, 0.9980, 0.9964,\n",
                            "        0.9945, 0.9979, 0.9985, 0.9971, 0.9972, 0.9960, 0.9980, 0.9963, 0.9965,\n",
                            "        0.9967, 0.9966, 0.9977, 0.9962, 0.9952, 0.9974, 0.9949, 0.9982, 0.9972,\n",
                            "        0.9931, 0.9973, 0.9962, 0.9957, 0.9966, 0.9980, 0.9970, 0.9945, 0.9987,\n",
                            "        0.9947, 0.9971, 0.9943, 0.9979, 0.9918, 0.9968, 0.9953, 0.9953, 0.9977,\n",
                            "        0.9972, 0.9961, 0.9966, 0.9967, 0.9977, 0.9984, 0.9960, 0.9951, 0.9943,\n",
                            "        0.9922, 0.9941, 0.9947, 0.9921, 0.9942, 0.9943, 0.9913, 0.9980, 0.9976,\n",
                            "        0.9990, 0.9900, 0.9965, 0.9981, 0.9891, 0.9988, 0.9983, 0.9939, 0.9933],\n",
                            "       device='cuda:0')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 160
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 144,
            "source": [
                "from pytorch_lightning.core.lightning import LightningModule\r\n",
                "from torchmetrics import Accuracy, F1\r\n",
                "from torch.optim import lr_scheduler\r\n",
                "\r\n",
                "class MultiTaskLearningModel(LightningModule):\r\n",
                "    def __init__(\r\n",
                "        self, base_model = None, dropout_rate: float = 0.1, \r\n",
                "        hidden_size: int = 768, \r\n",
                "        num_tokens_labels: int = 5\r\n",
                "    ):\r\n",
                "        super(MultiTaskLearningModel, self).__init__()\r\n",
                "        if base_model is None:\r\n",
                "            self.base_model = BertModel.from_pretrained(\"bert-base-cased\")\r\n",
                "        else:\r\n",
                "            self.base_model = base_model\r\n",
                "\r\n",
                "        self.num_tokens_labels = num_tokens_labels\r\n",
                "\r\n",
                "        # We could avoid sigmoid here and use the\r\n",
                "        # BCEWithLogitsLoss, which computes both the sigmoid\r\n",
                "        # and the BCE with a trick for numerical stability.\r\n",
                "        self.seq_clf = nn.Sequential(\r\n",
                "            nn.Dropout(p=dropout_rate),\r\n",
                "            nn.Linear(in_features=768, out_features=hidden_size),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(in_features=hidden_size, out_features=1),\r\n",
                "            nn.Sigmoid()\r\n",
                "        )\r\n",
                "\r\n",
                "        self.tokens_clf = nn.Sequential(\r\n",
                "            nn.Dropout(p=dropout_rate),\r\n",
                "            nn.Linear(hidden_size, num_tokens_labels)\r\n",
                "            # nn.LogSoftmax(dim=1)\r\n",
                "        )\r\n",
                "\r\n",
                "        self.multi_loss = MultiTaskLoss(2, num_tokens_labels)\r\n",
                "        self.seqc_accuracy = Accuracy()\r\n",
                "        self.tokc_accuracy = Accuracy(\r\n",
                "            ignore_index=(num_tokens_labels - 1)\r\n",
                "        )\r\n",
                "        self.seqc_f1 = F1()\r\n",
                "        self.tokc_f1 = F1(\r\n",
                "            average=\"macro\", \r\n",
                "            num_classes=num_tokens_labels, \r\n",
                "            ignore_index=(num_tokens_labels - 1)\r\n",
                "        )\r\n",
                "\r\n",
                "        for param in self.seq_clf.parameters():\r\n",
                "            if param.dim() > 1:\r\n",
                "                nn.init.xavier_uniform_(param)\r\n",
                "\r\n",
                "        for param in self.tokens_clf.parameters():\r\n",
                "            if param.dim() > 1:\r\n",
                "                nn.init.xavier_uniform_(param)\r\n",
                "        \r\n",
                "    def forward(self, input_ids, attention_mask):\r\n",
                "        output = self.base_model(\r\n",
                "            input_ids=input_ids,\r\n",
                "            attention_mask=attention_mask\r\n",
                "        )\r\n",
                "\r\n",
                "        last_hidden_state = output.last_hidden_state\r\n",
                "\r\n",
                "        # TODO Since we are using mean, maybe it's better not to pad to max?\r\n",
                "        seq_clf_out = self.seq_clf(torch.mean(last_hidden_state, dim=1))\r\n",
                "        tokens_clf_out = self.tokens_clf(last_hidden_state)\r\n",
                "\r\n",
                "        return seq_clf_out, tokens_clf_out\r\n",
                "\r\n",
                "    def configure_optimizers(self):\r\n",
                "        optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\r\n",
                "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=2)\r\n",
                "        return {\r\n",
                "            \"optimizer\": optimizer,\r\n",
                "            \"lr_scheduler\": {\r\n",
                "                \"scheduler\": scheduler,\r\n",
                "                \"interval\": \"epoch\",\r\n",
                "                \"monitor\": \"val_loss\"\r\n",
                "            }\r\n",
                "        }\r\n",
                "\r\n",
                "    def training_step(self, batch, batch_idx):\r\n",
                "        input_ids, attention_mask, tokens_labels, labels = batch\r\n",
                "        seqc_out, tokc_out = self(input_ids, attention_mask)\r\n",
                "        loss = self.multi_loss(seqc_out, tokc_out, labels, tokens_labels, attention_mask)\r\n",
                "        return loss\r\n",
                "\r\n",
                "    def validation_step(self, batch, batch_idx):\r\n",
                "        input_ids, attention_mask, tokens_labels, labels = batch\r\n",
                "        seqc_out, tokc_out = self(input_ids, attention_mask)\r\n",
                "        loss = self.multi_loss(seqc_out, tokc_out, labels, tokens_labels, attention_mask)\r\n",
                "\r\n",
                "        tokens_labels = torch.where(\r\n",
                "            tokens_labels == -100,\r\n",
                "            self.num_tokens_labels - 1, \r\n",
                "            tokens_labels\r\n",
                "        )\r\n",
                "\r\n",
                "        torch.save(tokc_out, \"tokc_out.pkl\")\r\n",
                "        torch.save(tokens_labels, \"tokens_labels.pkl\")\r\n",
                "\r\n",
                "        tokc_preds = torch.argmax(tokc_out, -1)\r\n",
                "\r\n",
                "        #seqc_acc = self.seqc_accuracy(seqc_out, labels.int())\r\n",
                "        tokc_acc = self.tokc_accuracy(tokc_preds, tokens_labels)\r\n",
                "        #seqc_f1 = self.seqc_f1(seqc_out, labels.int())\r\n",
                "        tokc_f1 = self.tokc_f1(tokc_preds.view(-1), tokens_labels.view(-1))\r\n",
                "\r\n",
                "        self.log(\"val_loss\", loss, prog_bar=True)\r\n",
                "        #self.log(\"val_seqc_acc\", seqc_acc, prog_bar=True, on_step=True, on_epoch=True)\r\n",
                "        self.log(\"val_tokc_acc\", tokc_acc, prog_bar=True, on_step=True, on_epoch=True)\r\n",
                "        #self.log(\"val_seqc_f1\", seqc_f1, prog_bar=True, on_step=True, on_epoch=True)\r\n",
                "        self.log(\"val_tokc_f1\", tokc_f1, prog_bar=True, on_step=True, on_epoch=True)\r\n",
                "\r\n",
                "        return loss\r\n",
                "\r\n",
                "    def test_step(self, batch, batch_idx):\r\n",
                "        input_ids, attention_mask, tokens_labels, labels = batch\r\n",
                "        seqc_out, tokc_out = self(input_ids, attention_mask)\r\n",
                "        loss = self.multi_loss(seqc_out, tokc_out, labels, tokens_labels, attention_mask)\r\n",
                "        return loss\r\n",
                "\r\n",
                "    def freeze_base(self):\r\n",
                "        for param in self.base_model.named_parameters():\r\n",
                "            param[1].requires_grad=False\r\n",
                "\r\n",
                "    def unfreeze_base(self):\r\n",
                "        for param in self.base_model.named_parameters():\r\n",
                "            param[1].requires_grad=True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "source": [
                "from pytorch_lightning import LightningDataModule\r\n",
                "\r\n",
                "class WikiDataModule(LightningDataModule):\r\n",
                "\r\n",
                "    def __init__(\r\n",
                "        self, data_dir: str = \"datasets/historical_events\",\r\n",
                "        batch_size: int = 128, num_workers: int = 0, \r\n",
                "        num_tokens_labels: int = 5\r\n",
                "    ):\r\n",
                "        super().__init__()\r\n",
                "        self.data_dir = data_dir\r\n",
                "        self.batch_size = batch_size\r\n",
                "        self.num_workers = num_workers\r\n",
                "\r\n",
                "        self.dims = 512\r\n",
                "        self.num_tokens_labels = num_tokens_labels\r\n",
                "\r\n",
                "        self.train_dict, self.valid_dict, self.test_dict = {}, {}, {}\r\n",
                "\r\n",
                "    def data_load_util(self, data_split: str):\r\n",
                "        return {\r\n",
                "            \"input_ids\": torch.load(f\"{self.data_dir}/{data_split}/input_ids.pkl\"),\r\n",
                "            \"attention_mask\": torch.load(f\"{self.data_dir}/{data_split}/attention_mask.pkl\"),\r\n",
                "            \"tokens_labels\": torch.load(f\"{self.data_dir}/{data_split}/tokens_labels.pkl\"),\r\n",
                "            \"labels\": torch.load(f\"{self.data_dir}/{data_split}/labels.pkl\"),\r\n",
                "            \"tag2idx\": torch.load(f\"{self.data_dir}/{data_split}/tag2idx.pkl\"),\r\n",
                "            \"idx2tag\": torch.load(f\"{self.data_dir}/{data_split}/idx2tag.pkl\")   \r\n",
                "        }\r\n",
                "\r\n",
                "    def prepare_data(self):\r\n",
                "        self.train_dict = self.data_load_util(\"train\")\r\n",
                "        self.valid_dict = self.data_load_util(\"valid\")\r\n",
                "        self.test_dict = self.data_load_util(\"test\")\r\n",
                "\r\n",
                "    def setup(self, stage=None):\r\n",
                "\r\n",
                "        # Assign train/val datasets for use in dataloaders\r\n",
                "        if stage == \"fit\" or stage is None:\r\n",
                "            self.wiki_train = TensorDataset(\r\n",
                "                torch.tensor(self.train_dict[\"input_ids\"]), \r\n",
                "                torch.tensor(self.train_dict[\"attention_mask\"]),\r\n",
                "                torch.tensor(self.train_dict[\"tokens_labels\"]),\r\n",
                "                torch.tensor(self.train_dict[\"labels\"], dtype=torch.float32)\r\n",
                "            )\r\n",
                "            self.wiki_valid = TensorDataset(\r\n",
                "                torch.tensor(self.valid_dict[\"input_ids\"]), \r\n",
                "                torch.tensor(self.valid_dict[\"attention_mask\"]),\r\n",
                "                torch.tensor(self.valid_dict[\"tokens_labels\"]),\r\n",
                "                torch.tensor(self.valid_dict[\"labels\"], dtype=torch.float32)\r\n",
                "            )\r\n",
                "\r\n",
                "        # Assign test dataset for use in dataloader(s)\r\n",
                "        if stage == \"test\" or stage is None:\r\n",
                "            self.wiki_test = TensorDataset(\r\n",
                "                torch.tensor(self.test_dict[\"input_ids\"]), \r\n",
                "                torch.tensor(self.test_dict[\"attention_mask\"]),\r\n",
                "                torch.tensor(self.test_dict[\"tokens_labels\"]),\r\n",
                "                torch.tensor(self.test_dict[\"labels\"], dtype=torch.float32)\r\n",
                "            )\r\n",
                "\r\n",
                "    def train_dataloader(self):\r\n",
                "        return DataLoader(\r\n",
                "            self.wiki_train, batch_size=self.batch_size, \r\n",
                "            num_workers=self.num_workers\r\n",
                "        )\r\n",
                "\r\n",
                "    def val_dataloader(self):\r\n",
                "        return DataLoader(\r\n",
                "            self.wiki_valid, batch_size=self.batch_size,\r\n",
                "            num_workers=self.num_workers\r\n",
                "        )\r\n",
                "\r\n",
                "    def test_dataloader(self):\r\n",
                "        return DataLoader(\r\n",
                "            self.wiki_test, batch_size=self.batch_size,\r\n",
                "            num_workers=self.num_workers\r\n",
                "        )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 130,
            "source": [
                "from torchmetrics import Accuracy, F1\r\n",
                "target = torch.tensor([0, 1, 2, 1])\r\n",
                "preds = torch.tensor([\r\n",
                "    [0.8, 0.1, 0.1], \r\n",
                "    [0.1, 0.8, 0.1],\r\n",
                "    [0.1, 0.1, 0.8],\r\n",
                "    [0.1, 0.8, 0.1]\r\n",
                "])\r\n",
                "accuracy = Accuracy()\r\n",
                "f1 = F1()\r\n",
                "accuracy(torch.argmax(preds, -1), target), f1(preds, target)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(tensor(1.), tensor(1.))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 130
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "from torchmetrics import Accuracy, F1\r\n",
                "target = torch.tensor([0, 1, 2, -100])\r\n",
                "target = map(lambda x: torch.tensor(3) if x == -100 else x, target)\r\n",
                "target = torch.tensor(list(target))\r\n",
                "preds = torch.tensor([\r\n",
                "    [0.8, 0.1, 0.1, 0], \r\n",
                "    [0.1, 0.8, 0.1, 0],\r\n",
                "    [0.1, 0.1, 0.8, 0],\r\n",
                "    [0.1, 0.8, 0.1, 0.1]\r\n",
                "])\r\n",
                "accuracy = Accuracy(ignore_index=3)\r\n",
                "f1 = F1(average=\"macro\", num_classes=4, ignore_index=3)\r\n",
                "accuracy(preds, target), f1(preds, target)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(tensor(1.), tensor(0.8889))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "from torchmetrics import Accuracy, F1\r\n",
                "target = torch.tensor([0, 1, 1, 0])\r\n",
                "target = map(lambda x: torch.tensor(3) if x == -100 else x, target)\r\n",
                "target = torch.tensor(list(target))\r\n",
                "preds = torch.tensor([0.1, 0.8, 0.8, 0.1])\r\n",
                "accuracy = Accuracy()\r\n",
                "f1 = F1()\r\n",
                "accuracy(preds, target), f1(preds, target)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(tensor(1.), tensor(1.))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 54
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "source": [
                "b = torch.tensor([\r\n",
                "    [0, 1, 2],\r\n",
                "    [0, 2, -100],\r\n",
                "    [-100, 2, 0]\r\n",
                "])\r\n",
                "b"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[   0,    1,    2],\n",
                            "        [   0,    2, -100],\n",
                            "        [-100,    2,    0]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 110
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 104,
            "source": [
                "c = torch.empty(b.shape).fill_(3)\r\n",
                "c"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[3., 3., 3.],\n",
                            "        [3., 3., 3.],\n",
                            "        [3., 3., 3.]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 104
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 112,
            "source": [
                "torch.where(b == -100, 3, b)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[0, 1, 2],\n",
                            "        [0, 2, 3],\n",
                            "        [3, 2, 0]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 112
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "valid_idx2tag = torch.load(\"datasets/historical_events/valid/idx2tag.pkl\")\r\n",
                "train_idx2tag = torch.load(\"datasets/historical_events/train/idx2tag.pkl\")\r\n",
                "test_idx2tag = torch.load(\"datasets/historical_events/test/idx2tag.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "test_idx2tag, valid_idx2tag, train_idx2tag"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "({0: 'B-hist', 1: 'B-not-hist', 2: 'I-hist', 3: 'I-not-hist', -100: 'MASK'},\n",
                            " {0: 'B-hist', 1: 'B-not-hist', 2: 'I-hist', 3: 'I-not-hist', -100: 'MASK'},\n",
                            " {0: 'B-hist', 1: 'B-not-hist', 2: 'I-hist', 3: 'I-not-hist', -100: 'MASK'})"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "source": [
                "a = torch.tensor(0.1)\r\n",
                "a"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor(0.1000)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 78
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "source": [
                "a = a.int()\r\n",
                "a"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor(0, dtype=torch.int32)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 83
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 145,
            "source": [
                "from pytorch_lightning import Trainer\r\n",
                "\r\n",
                "dm = WikiDataModule()\r\n",
                "model = MultiTaskLearningModel()\r\n",
                "trainer = Trainer()\r\n",
                "trainer.fit(model, dm)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
                        "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "GPU available: True, used: False\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "D:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1292: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": []
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\n",
                        "  | Name          | Type          | Params\n",
                        "------------------------------------------------\n",
                        "0 | base_model    | BertModel     | 108 M \n",
                        "1 | seq_clf       | Sequential    | 591 K \n",
                        "2 | tokens_clf    | Sequential    | 3.8 K \n",
                        "3 | multi_loss    | MultiTaskLoss | 2     \n",
                        "4 | seqc_accuracy | Accuracy      | 0     \n",
                        "5 | tokc_accuracy | Accuracy      | 0     \n",
                        "6 | seqc_f1       | F1            | 0     \n",
                        "7 | tokc_f1       | F1            | 0     \n",
                        "------------------------------------------------\n",
                        "108 M     Trainable params\n",
                        "0         Non-trainable params\n",
                        "108 M     Total params\n",
                        "435.622   Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "D:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "tokc_shape torch.Size([128, 512, 5]) tensor([[[-0.9654, -0.8255, -0.1501,  0.4268, -0.3652],\n",
                        "         [-1.2315, -0.2665,  0.0677,  0.5974,  0.2710],\n",
                        "         [ 0.6101, -0.0409,  0.4318,  1.9059,  0.6774],\n",
                        "         ...,\n",
                        "         [ 0.2781,  0.2673, -0.7821,  0.3398,  0.0379],\n",
                        "         [ 0.1401,  0.2081, -0.8498,  0.2469, -0.0970],\n",
                        "         [ 0.1687,  0.3741, -0.8724,  0.2815, -0.3217]],\n",
                        "\n",
                        "        [[-0.9416, -0.9621, -0.1640,  0.7597, -0.4565],\n",
                        "         [-1.3807, -0.4784,  0.1636,  0.4127, -0.0963],\n",
                        "         [-0.1872, -0.4662,  0.2936,  0.5350,  0.2583],\n",
                        "         ...,\n",
                        "         [ 0.1968,  0.3323,  0.2181,  0.6491, -0.2826],\n",
                        "         [-0.2214,  0.5467, -0.1261,  0.4248, -0.4629],\n",
                        "         [-0.0019,  0.4358, -0.0523,  0.5346, -0.3377]],\n",
                        "\n",
                        "        [[-1.0146, -0.5266, -0.5399,  0.3677, -0.3474],\n",
                        "         [-1.0747, -0.6257, -0.4775,  0.3459, -0.2881],\n",
                        "         [-0.4422,  0.4452,  0.1168,  0.3480, -0.3033],\n",
                        "         ...,\n",
                        "         [-0.0811,  0.6340, -0.1366,  0.5074,  0.2369],\n",
                        "         [-0.0967,  0.5983, -0.0380,  0.6364,  0.3280],\n",
                        "         [-0.0694,  0.5622, -0.1365,  0.6226,  0.3145]],\n",
                        "\n",
                        "        ...,\n",
                        "\n",
                        "        [[-0.6640, -0.6347, -0.3610,  0.4874, -0.5911],\n",
                        "         [-1.3445, -0.2467, -0.0204,  0.4785, -0.1249],\n",
                        "         [ 0.2525,  0.4230, -0.5251,  1.3178, -0.2443],\n",
                        "         ...,\n",
                        "         [ 0.0928,  0.9400, -0.4412,  0.8857, -0.2895],\n",
                        "         [-0.2225,  1.1014, -0.1365,  0.6400, -0.3422],\n",
                        "         [-0.0854,  0.9057,  0.0097,  0.5781, -0.2180]],\n",
                        "\n",
                        "        [[-0.9445, -0.0329, -0.3667,  0.8134, -0.2934],\n",
                        "         [-0.9716, -0.0346, -0.3582,  0.4345,  0.1399],\n",
                        "         [ 0.4338,  1.0631, -0.4117,  0.5599,  0.4250],\n",
                        "         ...,\n",
                        "         [ 0.1422,  1.2687, -0.4438,  0.4937, -0.1233],\n",
                        "         [ 0.3543,  1.4546, -0.6253,  0.5176, -0.1514],\n",
                        "         [ 0.3011,  1.3455, -0.6586,  0.4601, -0.2725]],\n",
                        "\n",
                        "        [[-1.1027, -0.3077,  0.0724,  0.5146, -0.5333],\n",
                        "         [-1.3423, -0.4656,  0.2274,  0.2401,  0.0058],\n",
                        "         [ 0.3793, -0.6684, -0.2065,  0.4864,  0.3370],\n",
                        "         ...,\n",
                        "         [ 0.2050,  0.6160, -0.5107,  0.3471, -0.2624],\n",
                        "         [ 0.0890,  0.4043, -0.5322,  0.2849, -0.1451],\n",
                        "         [ 0.1986,  0.3527, -0.6993,  0.3078, -0.2969]]])\n",
                        "tokens_labels_shape torch.Size([128, 512]) tensor([[4, 4, 4,  ..., 4, 4, 4],\n",
                        "        [4, 4, 4,  ..., 4, 4, 4],\n",
                        "        [4, 4, 4,  ..., 4, 4, 4],\n",
                        "        ...,\n",
                        "        [4, 4, 4,  ..., 4, 4, 4],\n",
                        "        [4, 4, 4,  ..., 4, 4, 4],\n",
                        "        [4, 4, 4,  ..., 4, 4, 4]])\n",
                        "Validation sanity check:  50%|█████     | 1/2 [01:26<01:26, 86.85s/it]"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 133,
            "source": [
                "from torchmetrics import Accuracy, F1\r\n",
                "\r\n",
                "tokc_out = torch.load(\"tokc_out.pkl\")\r\n",
                "tokens_labels = torch.load(\"tokens_labels.pkl\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 127,
            "source": [
                "tokc_out.shape, tokens_labels.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(torch.Size([128, 512, 5]), torch.Size([128, 512]))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 127
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 140,
            "source": [
                "torch.argmax(tokc_out, -1).shape, tokens_labels.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(torch.Size([128, 512]), torch.Size([128, 512]))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 140
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 141,
            "source": [
                "acc = Accuracy()\r\n",
                "f1 = F1()\r\n",
                "f1(torch.argmax(tokc_out, -1).view(-1), tokens_labels.view(-1))"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor(0.5858)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 141
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 143,
            "source": [
                "targets = torch.tensor([\r\n",
                "    [1, 2, 0],\r\n",
                "    [0, 1, 1],\r\n",
                "    [0, 0, 2]   \r\n",
                "]\r\n",
                ")\r\n",
                "\r\n",
                "preds = torch.tensor([\r\n",
                "    [1, 2, 0],\r\n",
                "    [0, 1, 1],\r\n",
                "    [0, 0, 2]   \r\n",
                "])\r\n",
                "\r\n",
                "f1(preds.view(-1), targets.view(-1))"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor(1.)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 143
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit ('tf_p3.9': conda)"
        },
        "interpreter": {
            "hash": "f3a0b09ceef9b827a17ce91fbca1b0359a993a122e166af5f0b6d31a5625f693"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}