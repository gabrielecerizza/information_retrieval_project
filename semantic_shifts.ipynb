{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import numpy as np\r\n",
                "import torch\r\n",
                "from gensim.models import KeyedVectors\r\n",
                "from irproject.semantic_shifts import (\r\n",
                "    compute_cosine_shifts, compute_freqs, \r\n",
                "    compute_nn_shifts, compute_senses_frequencies, \r\n",
                "    compute_targets_embeddings, evaluate_shifts, \r\n",
                "    get_autoencoded_embeddings, get_most_freq_targets, \r\n",
                "    get_targets, get_sentences_with_targets, \r\n",
                "    get_umap_embeddings, intersect_vocabulary, \r\n",
                "    load_data, load_target_embeddings, \r\n",
                "    load_target_sentences_indices, load_semeval_targets, \r\n",
                "    perform_clustering, plot_targets_senses, \r\n",
                "    procrustes_align_gensim, save_context_emb_results, \r\n",
                "    save_static_emb_results, save_targets_embeddings, \r\n",
                "    tokenize_sentences\r\n",
                ")\r\n",
                "from scipy.spatial import distance\r\n",
                "from tqdm.auto import tqdm\r\n",
                "from transformers import AutoTokenizer, AutoModel"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "np.random.seed(42)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Semantic shifts"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Orthogonal Procrustes Approach"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model_old = KeyedVectors.load_word2vec_format(\r\n",
                "    \"datasets/histo-fast-300d.bin\", binary=True\r\n",
                ")\r\n",
                "model_new = KeyedVectors.load_word2vec_format(\r\n",
                "    \"datasets/wiki-news-300d-1M.vec\"\r\n",
                ")\r\n",
                "\r\n",
                "semeval_targets = load_semeval_targets(\"datasets/semeval2020/\")\r\n",
                "targets = get_most_freq_targets(model_old, model_new, semeval_targets)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model_new = procrustes_align_gensim(model_old, model_new, targets)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "op_results = compute_cosine_shifts(model_old, model_new, targets)\r\n",
                "list(op_results.items())[:15]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "np.round(evaluate_shifts(op_results)[0], 3)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "save_static_emb_results(\r\n",
                "    op_results, model_old, model_new, \"orthogonal_procrustes\"\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Nearest Neighbors Approach "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "With this approach we do not need the alignment we performed with Orthogonal Procrustes."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model_old = KeyedVectors.load_word2vec_format(\r\n",
                "    \"datasets/histo-fast-300d.bin\", binary=True\r\n",
                ")\r\n",
                "model_new = KeyedVectors.load_word2vec_format(\r\n",
                "    \"datasets/wiki-news-300d-1M.vec\"\r\n",
                ")\r\n",
                "\r\n",
                "semeval_targets = load_semeval_targets(\"datasets/semeval2020/\")\r\n",
                "targets = get_most_freq_targets(model_old, model_new, semeval_targets)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model_old, model_new = intersect_vocabulary(\r\n",
                "    model_old, model_new\r\n",
                ")\r\n",
                "nn_results = compute_nn_shifts(model_old, model_new, targets, topn=15)\r\n",
                "list(nn_results.items())[:15]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "np.round(evaluate_shifts(nn_results)[0], 3)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "save_static_emb_results(\r\n",
                "    nn_results, model_old, model_new, \"nearest_neighbors\"\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Jensen Shannon Distance Approach"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "data = load_data(\"datasets/semeval2020/\")\r\n",
                "freqs_dict = compute_freqs(data)\r\n",
                "data[\"targets_tagged\"], data[\"targets_clean\"] = get_targets(data, freqs_dict)\r\n",
                "sentences_with_trg = get_sentences_with_targets(\r\n",
                "    data, data[\"targets_tagged\"]\r\n",
                ")\r\n",
                "\r\n",
                "corpora_names = [\"corpus_old\", \"corpus_new\"]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# We need to truncate because we have a sentence\r\n",
                "# with 524 tokens, which is greater than 512, the\r\n",
                "# maximum number of tokens allowed by the \r\n",
                "# pretrained BERT model.\r\n",
                "tokenizer = AutoTokenizer.from_pretrained(\r\n",
                "    \"pretrained/bert-semeval2020-tokenizer\",\r\n",
                "    truncation=True, max_length=512\r\n",
                ")\r\n",
                "model = AutoModel.from_pretrained(\r\n",
                "    \"pretrained/bert-semeval2020\"\r\n",
                ")\r\n",
                "model.to(device)\r\n",
                "model.eval()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Here we tokenize the sentences and we compute the embeddings. This is a memory intensive step. Systems with less than 16GB of RAM will crash. Consider computing the embeddings for the two corpora with two different cells, making sure to have enough memory, for instance by triggering the garbage collector.\r\n",
                "\r\n",
                "We are saving the embeddings on disk after the computation, so we can skip this cell on subsequent executions."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for corpus_name in tqdm(\r\n",
                "    corpora_names,\r\n",
                "    desc=\"Tokenizing corpus\",\r\n",
                "    leave=False\r\n",
                "):\r\n",
                "    tokenized_sentences = tokenize_sentences(\r\n",
                "        sentences_with_trg[corpus_name], \r\n",
                "        tokenizer,\r\n",
                "        data[\"targets_tagged\"],\r\n",
                "        data[\"targets_clean\"]\r\n",
                "    )\r\n",
                "\r\n",
                "    targets_embeddings_dict, targets_sentences_dict = compute_targets_embeddings(\r\n",
                "        model, tokenizer, corpus_name, data, tokenized_sentences\r\n",
                "    )\r\n",
                "\r\n",
                "    save_targets_embeddings(\r\n",
                "        targets_embeddings_dict, \r\n",
                "        targets_sentences_dict,\r\n",
                "        corpus_name\r\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Here we process the embeddings of each target word. First, we pass the embeddings through an autoencoder. Then we further reduce the dimensionality of the embeddings by using UMAP. Finally, we cluster the embeddings with HDBSCAN, estimate the frequencies of the senses (clusters) and compute the Jensen Shannon Distance between the frequencies of the senses in the two epochs (corpora)."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from irproject.semantic_shifts import load_semeval_targets\r\n",
                "\r\n",
                "semeval_targets = load_semeval_targets(\r\n",
                "    \"datasets/semeval2020\", remove_tags=False\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from collections import Counter\r\n",
                "\r\n",
                "results = dict()\r\n",
                "top_sentences = dict()\r\n",
                "targets_senses_frequencies = dict()\r\n",
                "targets_senses_counts = dict()\r\n",
                "num_sentences = 3\r\n",
                "\r\n",
                "one_epoch_targets = []\r\n",
                "hdbscan_errors = []\r\n",
                "memory_issues = []\r\n",
                "\r\n",
                "for target_idx, target in tqdm(\r\n",
                "    enumerate(data[\"targets_tagged\"]),\r\n",
                "    # enumerate(semeval_targets),\r\n",
                "    desc=\"Computing score for targets\",\r\n",
                "    leave=False\r\n",
                "):\r\n",
                "    # Skip the proper nouns, since they are too noisy.\r\n",
                "    if target[-2:] == \"pn\":\r\n",
                "        continue\r\n",
                "\r\n",
                "    targets_senses_frequencies[target] = dict()\r\n",
                "    targets_senses_counts[target] = dict()\r\n",
                "\r\n",
                "    embeddings = []\r\n",
                "    embeddings_num_per_epoch = []\r\n",
                "    for corpus_name in corpora_names:\r\n",
                "        corpus_embeddings = load_target_embeddings(target, corpus_name)\r\n",
                "        embeddings.append(corpus_embeddings)\r\n",
                "        embeddings_num_per_epoch.append(len(corpus_embeddings))\r\n",
                "\r\n",
                "    try:\r\n",
                "        X = np.vstack(embeddings)\r\n",
                "    except:\r\n",
                "        # The target has embeddings only for one\r\n",
                "        # corpus. \r\n",
                "        one_epoch_targets.append(target)\r\n",
                "        continue\r\n",
                "\r\n",
                "    if X.shape[0] > 27500:\r\n",
                "        # Unable to further compute these\r\n",
                "        # targets with 16GB of RAM due to\r\n",
                "        # high number of embeddings. \r\n",
                "        memory_issues.append(target)\r\n",
                "\r\n",
                "    if target in memory_issues:\r\n",
                "        continue\r\n",
                "    \r\n",
                "    X = get_autoencoded_embeddings(X, target)\r\n",
                "\r\n",
                "    embeddings_epochs = []\r\n",
                "    embeddings_epochs.extend([0] * embeddings_num_per_epoch[0])\r\n",
                "    embeddings_epochs.extend([1] * embeddings_num_per_epoch[1])\r\n",
                "\r\n",
                "    X = np.vstack(\r\n",
                "        # Autoencoded embeddings old \r\n",
                "        # and autoencoded embeddings new.\r\n",
                "        [X[:embeddings_num_per_epoch[0]], \r\n",
                "        X[embeddings_num_per_epoch[0]:]]\r\n",
                "    )\r\n",
                "    X = get_umap_embeddings(X, target)\r\n",
                "\r\n",
                "    try:\r\n",
                "        # min_cluster_size = int(0.3 * len(X))\r\n",
                "        min_cluster_size = max(\r\n",
                "            # min(80, int(0.09770099572992251 * len(X))), 2\r\n",
                "            min(80, int(0.1 * len(X))), 2\r\n",
                "        )\r\n",
                "        labels, probas = perform_clustering(X, min_cluster_size)\r\n",
                "    except:\r\n",
                "        hdbscan_errors.append(target)\r\n",
                "        continue\r\n",
                "\r\n",
                "    senses_frequencies = compute_senses_frequencies(\r\n",
                "        labels, embeddings_epochs, embeddings_num_per_epoch\r\n",
                "    )\r\n",
                "\r\n",
                "    # Key \"0\" in senses_frequencies is for corpus_old, \r\n",
                "    # while key \"1\" is for corpus_new.\r\n",
                "    targets_senses_frequencies[target][\"corpus_old\"] = senses_frequencies[0]\r\n",
                "    targets_senses_frequencies[target][\"corpus_new\"] = senses_frequencies[1]\r\n",
                "\r\n",
                "    old_counts = Counter(labels[:embeddings_num_per_epoch[0]])\r\n",
                "    old_counts = {\r\n",
                "        int(key): val for key, val in old_counts.items()\r\n",
                "    }\r\n",
                "    old_counts = dict(\r\n",
                "        sorted(\r\n",
                "            old_counts.items(), \r\n",
                "            key=lambda item: item[0],\r\n",
                "        )\r\n",
                "    )\r\n",
                "    new_counts = Counter(labels[embeddings_num_per_epoch[0]:])\r\n",
                "    new_counts = {\r\n",
                "        int(key): val for key, val in new_counts.items()\r\n",
                "    }\r\n",
                "    new_counts = dict(\r\n",
                "        sorted(\r\n",
                "            new_counts.items(), \r\n",
                "            key=lambda item: item[0],\r\n",
                "        )\r\n",
                "    )\r\n",
                "    \r\n",
                "    targets_senses_counts[target][\"corpus_old\"] = old_counts\r\n",
                "    targets_senses_counts[target][\"corpus_new\"] = new_counts\r\n",
                "    sentences_num_per_sense = Counter(labels)\r\n",
                "\r\n",
                "    jsd = distance.jensenshannon(\r\n",
                "        list(senses_frequencies[0].values()), \r\n",
                "        list(senses_frequencies[1].values()), \r\n",
                "        2.0\r\n",
                "    )\r\n",
                "\r\n",
                "    results[target] = jsd\r\n",
                "    top_sentences[target] = []\r\n",
                "\r\n",
                "    # Here we get the sentences for each sense.\r\n",
                "    for label in set(labels):\r\n",
                "        if label == -1:\r\n",
                "            # We skip the \"noise\" label.\r\n",
                "            continue\r\n",
                "\r\n",
                "        label_probas = []\r\n",
                "        for i, x_label in enumerate(labels):\r\n",
                "            if x_label == label:\r\n",
                "                label_probas.append((i, probas[i]))\r\n",
                "\r\n",
                "        label_probas = sorted(\r\n",
                "            label_probas, key=lambda item: item[1], reverse=True\r\n",
                "        )\r\n",
                "        top_label_sentences = []\r\n",
                "        for j in range(\r\n",
                "            min(\r\n",
                "                num_sentences, \r\n",
                "                sentences_num_per_sense[label]\r\n",
                "            )\r\n",
                "        ):\r\n",
                "            idx, _ = label_probas[j]\r\n",
                "            if embeddings_epochs[idx] == 0:\r\n",
                "                corpus_name = \"corpus_old\"\r\n",
                "                idx_in_epoch = idx\r\n",
                "            else:\r\n",
                "                corpus_name = \"corpus_new\"\r\n",
                "                # Each embedding has its own sentence. We have\r\n",
                "                # stacked the embeddings of the two epochs. So,\r\n",
                "                # if we want the sentence of the second embedding\r\n",
                "                # of the \"corpus_new\", we need to take into account\r\n",
                "                # that idx will not be 2, since we have the\r\n",
                "                # embeddings of the \"corpus_old\" first. \r\n",
                "                idx_in_epoch = idx - embeddings_num_per_epoch[0]\r\n",
                "\r\n",
                "            sentences_indices = load_target_sentences_indices(\r\n",
                "                target, corpus_name\r\n",
                "            )\r\n",
                "            sentence_idx = sentences_indices[idx_in_epoch]\r\n",
                "            sentence = sentences_with_trg[corpus_name][sentence_idx]\r\n",
                "            top_label_sentences.append(sentence)\r\n",
                "\r\n",
                "        top_sentences[target].append(top_label_sentences)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "np.round(evaluate_shifts(results, remove_tags=False)[0], 3)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "save_context_emb_results(\r\n",
                "    results, top_sentences, one_epoch_targets, \r\n",
                "    hdbscan_errors, memory_issues, targets_senses_frequencies,\r\n",
                "    targets_senses_counts, fname=\"jensen_shannon\"\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plot_targets_senses(fname=\"jensen_shannon\")"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit ('tf_p3.9': conda)"
        },
        "interpreter": {
            "hash": "f3a0b09ceef9b827a17ce91fbca1b0359a993a122e166af5f0b6d31a5625f693"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}