@inproceedings{schlechtweg-etal-2020-semeval,
    title = "{S}em{E}val-2020 Task 1: Unsupervised Lexical Semantic Change Detection",
    author = "Schlechtweg, Dominik  and
      McGillivray, Barbara  and
      Hengchen, Simon  and
      Dubossarsky, Haim  and
      Tahmasebi, Nina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.1",
    pages = "1--23",
    abstract = "Lexical Semantic Change detection, i.e., the task of identifying words that change meaning over time, is a very active research area, with applications in NLP, lexicography, and linguistics. Evaluation is currently the most pressing problem in Lexical Semantic Change detection, as no gold standards are available to the community, which hinders progress. We present the results of the first shared task that addresses this gap by providing researchers with an evaluation framework and manually annotated, high-quality datasets for English, German, Latin, and Swedish. 33 teams submitted 186 systems, which were evaluated on two subtasks.",
}

@inproceedings{basile-etal-2020-diacrita,
    title = "DIACR-Ita @ EVALITA2020: Overview of the EVALITA2020 Diachronic Lexical Semantics (DIACR-Ita) Task",
    author = "Pierpaolo Basile and Annalina Caputo and Tommaso Caselli and Pierluigi Cassotti and Rossella Varvara",
    year = "2020",
    language = "English",
    editor = "Valerio Basile and Danilo Croce and {Di Maro}, Maria and Passaro, {Lucia C.}",
    booktitle = "Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020)",
    publisher = "CEUR Workshop Proceedings (CEUR-WS.org)",
    note = "Evaluation Campaign of Natural Language Processing and Speech Tools for Italian, EVALITA 2020 ; Conference date: 17-12-2020",
}

@inproceedings{mikolov-etal-2013-word2vec,
    author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Distributed Representations of Words and Phrases and their Compositionality},
    volume = {26},
    year = {2013}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    publisher = "Association for Computational Linguistics",
    pages = "1532--1543",
}

@article{bojanowski-etal-2017-fasttext,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    pages = "135--146",
    abstract = "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
}

@inproceedings{hamilton-etal-2016-diachronic,
    title = "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
    author = "Hamilton, William L.  and
      Leskovec, Jure  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    publisher = "Association for Computational Linguistics",
    pages = "1489--1501",
}

@inproceedings{kutuzov-etal-2018-diachronic,
    title = "Diachronic word embeddings and semantic shifts: a survey",
    author = "Kutuzov, Andrey  and
      {\O}vrelid, Lilja  and
      Szymanski, Terrence  and
      Velldal, Erik",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    publisher = "Association for Computational Linguistics",
    pages = "1384--1397",
    abstract = "Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.",
}

@article{tahmasebi-etal-2018-survey,
    author    = {Nina Tahmasebi and
                Lars Borin and
                Adam Jatowt},
    title     = {Survey of Computational Approaches to Diachronic Conceptual Change},
    journal   = {CoRR},
    year      = {2018},
    url       = {http://arxiv.org/abs/1811.06278},
    archivePrefix = {arXiv},
    eprint    = {1811.06278},
    timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
}

@article{devlin-etal-2018-bert,
    author    = {Jacob Devlin and
                Ming{-}Wei Chang and
                Kenton Lee and
                Kristina Toutanova},
    title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                Understanding},
    journal   = {CoRR},
    year      = {2018},
    url       = {http://arxiv.org/abs/1810.04805},
    archivePrefix = {arXiv},
    eprint    = {1810.04805},
    timestamp = {Tue, 30 Oct 2018 20:39:56 +0100}
}

@inproceedings{dubossarsky-etal-2019-time,
    title = "Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change",
    author = "Dubossarsky, Haim  and
      Hengchen, Simon  and
      Tahmasebi, Nina  and
      Schlechtweg, Dominik",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1044",
    doi = "10.18653/v1/P19-1044",
    pages = "457--470",
    abstract = "State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this noise. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases.",
}

@inproceedings{giulianelli-etal-2020-analysing,
    title = "Analysing Lexical Semantic Change with Contextualised Word Representations",
    author = "Giulianelli, Mario  and
      Del Tredici, Marco  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.365",
    doi = "10.18653/v1/2020.acl-main.365",
    pages = "3960--3973",
    abstract = "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.",
}

@inproceedings{rother-etal-2020-cmce,
    title = "{CMCE} at {S}em{E}val-2020 Task 1: Clustering on Manifolds of Contextualized Embeddings to Detect Historical Meaning Shifts",
    author = "Rother, David  and
      Haider, Thomas  and
      Eger, Steffen",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.22",
    pages = "187--193",
    abstract = "This paper describes the system Clustering on Manifolds of Contextualized Embeddings (CMCE) submitted to the SemEval-2020 Task 1 on Unsupervised Lexical Semantic Change Detection. Subtask 1 asks to identify whether or not a word gained/lost a sense across two time periods. Subtask 2 is about computing a ranking of words according to the amount of change their senses underwent. Our system uses contextualized word embeddings from MBERT, whose dimensionality we reduce with an autoencoder and the UMAP algorithm, to be able to use a wider array of clustering algorithms that can automatically determine the number of clusters. We use Hierarchical Density Based Clustering (HDBSCAN) and compare it to Gaussian MixtureModels (GMMs) and other clustering algorithms. Remarkably, with only 10 dimensional MBERT embeddings (reduced from the original size of 768), our submitted model performs best on subtask 1 for English and ranks third in subtask 2 for English. In addition to describing our system, we discuss our hyperparameter configurations and examine why our system lags behind for the other languages involved in the shared task (German, Swedish, Latin). Our code is available at https://github.com/DavidRother/semeval2020-task1",
}

@article{laicher-etal-2021-explaining,
  author    = {Severin Laicher and
               Sinan Kurtyigit and
               Dominik Schlechtweg and
               Jonas Kuhn and
               Sabine Schulte im Walde},
  title     = {Explaining and Improving {BERT} Performance on Lexical Semantic Change
               Detection},
  journal   = {CoRR},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.07259},
  archivePrefix = {arXiv},
  eprint    = {2103.07259},
  timestamp = {Tue, 23 Mar 2021 16:29:47 +0100}
}

@article{sprugnoli-tonelli-2019-histo,
    author = {Sprugnoli, Rachele and Tonelli, Sara},
    title = "{Novel Event Detection and Classification for Historical Texts}",
    journal = {Computational Linguistics},
    volume = {45},
    number = {2},
    pages = {229-265},
    year = {2019},
    month = {06},
    abstract = "{Event processing is an active area of research in the Natural Language Processing community, but resources and automatic systems developed so far have mainly addressed contemporary texts. However, the recognition and elaboration of events is a crucial step when dealing with historical texts Particularly in the current era of massive digitization of historical sources: Research in this domain can lead to the development of methodologies and tools that can assist historians in enhancing their work, while having an impact also on the field of Natural Language Processing. Our work aims at shedding light on the complex concept of events when dealing with historical texts. More specifically, we introduce new annotation guidelines for event mentions and types, categorized into 22 classes. Then, we annotate a historical corpus accordingly, and compare two approaches for automatic event detection and classification following this novel scheme. We believe that this work can foster research in a field of inquiry as yet underestimated in the area of Temporal Information Processing. To this end, we release new annotation guidelines, a corpus, and new models for automatic annotation.}",
    issn = {0891-2017}
}

@inproceedings{martinc-etal-2020-leveraging,
    title = "Leveraging Contextual Embeddings for Detecting Diachronic Semantic Shift",
    author = "Martinc, Matej  and
      Kralj Novak, Petra  and
      Pollak, Senja",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    publisher = "European Language Resources Association",
    pages = "4811--4819",
    abstract = "We propose a new method that leverages contextual embeddings for the task of diachronic semantic shift detection by generating time specific word representations from BERT embeddings. The results of our experiments in the domain specific LiverpoolFC corpus suggest that the proposed method has performance comparable to the current state-of-the-art without requiring any time consuming domain adaptation on large corpora. The results on the newly created Brexit news corpus suggest that the method can be successfully used for the detection of a short-term yearly semantic shift. And lastly, the model also shows promising results in a multilingual settings, where the task was to detect differences and similarities between diachronic semantic shifts in different languages.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{mcconville-etal-2019-n2d,
  author    = {Ryan McConville and
               Ra{\'{u}}l Santos{-}Rodr{\'{\i}}guez and
               Robert J. Piechocki and
               Ian Craddock},
  title     = {{N2D:} (Not Too) Deep Clustering via Clustering the Local Manifold
               of an Autoencoded Embedding},
  journal   = {CoRR},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.05968},
  archivePrefix = {arXiv},
  eprint    = {1908.05968},
  timestamp = {Mon, 26 Aug 2019 08:49:30 +0200}
}

@InProceedings{campello-etal-2013-hdbscan,
author="Campello, Ricardo J. G. B.
and Moulavi, Davoud
and Sander, Joerg",
editor="Pei, Jian
and Tseng, Vincent S.
and Cao, Longbing
and Motoda, Hiroshi
and Xu, Guandong",
title="Density-Based Clustering Based on Hierarchical Density Estimates",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="160--172",
abstract="We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.",
isbn="978-3-642-37456-2"
}
